#include <opencv2/opencv.hpp>
#include <pangolin/pangolin.h>
#include "core/tensorrt_inference.h"
#include <iostream>

constexpr float SCORE_THRESHOLD = 0.99f;
constexpr int IMAGE_WIDTH = 1241;
constexpr int IMAGE_HEIGHT = 376;
constexpr int MODEL_IMAGE_SIZE = 1024;

cv::Mat load_calibration(const std::string& path) {
    std::ifstream file(path);
    std::string line;
    while (std::getline(file, line)) {
        if (line.substr(0, 2) == "P0") {
            std::istringstream iss(line.substr(4));
            cv::Mat P0(3, 4, CV_64F);
            for (int i = 0; i < 12; ++i) iss >> P0.at<double>(i / 4, i % 4);
            return P0(cv::Rect(0, 0, 3, 3));
        }
    }
    std::cerr << "P0 not found\n";
    exit(-1);
}

std::vector<cv::Mat> load_poses(const std::string& path, int num_poses) {
    std::ifstream file(path);
    std::vector<cv::Mat> poses;
    std::string line;
    for (int i = 0; i < num_poses && std::getline(file, line); ++i) {
        std::istringstream iss(line);
        cv::Mat pose(3, 4, CV_64F);
        for (int j = 0; j < 12; ++j) iss >> pose.at<double>(j / 4, j % 4);
        poses.push_back(pose);
    }
    if (poses.size() < num_poses) {
        std::cerr << "Failed to load " << num_poses << " poses\n";
        exit(-1);
    }
    return poses;
}

void estimate_pose(const std::vector<cv::Point2f>& points1, const std::vector<cv::Point2f>& points2, const cv::Mat& K,
                  cv::Mat& R, cv::Mat& T) {
    cv::Mat mask;
    cv::Mat E = cv::findEssentialMat(points1, points2, K, cv::RANSAC, 0.999, 1.0, mask);
    cv::recoverPose(E, points1, points2, K, R, T, mask);
    T = -R.t() * T;
    R = R.inv();  
}

double compute_rotation_error(const cv::Mat& R_est, const cv::Mat& R_gt) {
    double trace = cv::trace(R_est.t() * R_gt)[0];
    return acos(std::clamp((trace - 1.0) / 2.0, -1.0, 1.0)) * 180.0 / CV_PI;
}

double compute_translation_error(const cv::Mat& T_est, const cv::Mat& T_gt) {
    cv::Mat T_est_3 = T_est(cv::Rect(0, 0, 1, 3));  // Ensure 3x1
    cv::Mat T_gt_3 = T_gt(cv::Rect(0, 0, 1, 3));    // Ensure 3x1
    double cos_phi = T_est_3.dot(T_gt_3) / (cv::norm(T_est_3) * cv::norm(T_gt_3));
    return acos(std::clamp(cos_phi, -1.0, 1.0)) * 180.0 / CV_PI;
}

void draw_frustum(float scale, float r, float g, float b) {
    glLineWidth(2.0f);
    glBegin(GL_LINES);
    glColor3f(r, g, b);
    float z = 0.8f * scale, s = 0.5f * scale;
    glVertex3f(-s, -s, z); glVertex3f(s, -s, z);
    glVertex3f(s, -s, z);  glVertex3f(s, s, z);
    glVertex3f(s, s, z);   glVertex3f(-s, s, z);
    glVertex3f(-s, s, z);  glVertex3f(-s, -s, z);
    glVertex3f(0, 0, 0);   glVertex3f(-s, -s, z);
    glVertex3f(0, 0, 0);   glVertex3f(s, -s, z);
    glVertex3f(0, 0, 0);   glVertex3f(s, s, z);
    glVertex3f(0, 0, 0);   glVertex3f(-s, s, z);
    glEnd();
}

void visualize_poses(const cv::Mat& R_est, const cv::Mat& T_est, const cv::Mat& R_gt, const cv::Mat& T_gt) {
    pangolin::CreateWindowAndBind("Pose Visualization", 1024, 768);
    glEnable(GL_DEPTH_TEST);
    pangolin::OpenGlRenderState s_cam(
        pangolin::ProjectionMatrix(1024, 768, 500, 500, 512, 389, 0.1, 1000),
        pangolin::ModelViewLookAt(0, -10, -20, 0, 0, 0, 0.0, -1.0, 0.0)
    );
    pangolin::View& d_cam = pangolin::CreateDisplay()
        .SetBounds(0.0, 1.0, 0.0, 1.0, -1024.0f / 768.0f)
        .SetHandler(new pangolin::Handler3D(s_cam));

    while (!pangolin::ShouldQuit()) {
        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
        d_cam.Activate(s_cam);

        draw_frustum(1.0f, 0.0f, 0.0f, 1.0f);  // First camera (blue)

        float T_est_mat[16] = {
            float(R_est.at<double>(0,0)), float(R_est.at<double>(1,0)), float(R_est.at<double>(2,0)), 0,
            float(R_est.at<double>(0,1)), float(R_est.at<double>(1,1)), float(R_est.at<double>(2,1)), 0,
            float(R_est.at<double>(0,2)), float(R_est.at<double>(1,2)), float(R_est.at<double>(2,2)), 0,
            float(T_est.at<double>(0)), float(T_est.at<double>(1)), float(T_est.at<double>(2)), 1
        };
        glPushMatrix();
        glMultMatrixf(T_est_mat);
        draw_frustum(1.0f, 1.0f, 0.0f, 0.0f);  // Estimated (red)
        glPopMatrix();

        float T_gt_mat[16] = {
            float(R_gt.at<double>(0,0)), float(R_gt.at<double>(1,0)), float(R_gt.at<double>(2,0)), 0,
            float(R_gt.at<double>(0,1)), float(R_gt.at<double>(1,1)), float(R_gt.at<double>(2,1)), 0,
            float(R_gt.at<double>(0,2)), float(R_gt.at<double>(1,2)), float(R_gt.at<double>(2,2)), 0,
            float(T_gt.at<double>(0)), float(T_gt.at<double>(1)), float(T_gt.at<double>(2)), 1
        };
        glPushMatrix();
        glMultMatrixf(T_gt_mat);
        draw_frustum(1.0f, 0.0f, 1.0f, 0.0f);  // Ground truth (green)
        glPopMatrix();

        pangolin::FinishFrame();
    }
}

int main() {
    std::string dir_path = "/home/tomato/Downloads/data_odometry_gray/dataset/sequences/00/";
    TensorRTInference infer("../third_party/Superpoint_Lightglue/superpoint_lightglue_pipeline.trt.onnx",
                            "../third_party/Superpoint_Lightglue/superpoint_lightglue_pipeline.trt");

    std::vector<int64_t> keypoints, matches;
    std::vector<float> scores;
    infer.runInference(dir_path + "image_0/000000.png", dir_path + "image_0/000001.png", keypoints, matches, scores);

    float scale_x = float(IMAGE_WIDTH) / MODEL_IMAGE_SIZE;
    float scale_y = float(IMAGE_HEIGHT) / MODEL_IMAGE_SIZE;
    std::vector<cv::Point2f> points1, points2;
    for (size_t m = 0; m < scores.size() && m * 3 + 2 < matches.size(); ++m) {
        if (scores[m] >= SCORE_THRESHOLD && matches[m * 3] == 0) {
            int left_idx = matches[m * 3 + 1];
            int right_idx = matches[m * 3 + 2];
            if (left_idx >= 0 && left_idx < MODEL_IMAGE_SIZE && right_idx >= 0 && right_idx < MODEL_IMAGE_SIZE) {
                points1.emplace_back(keypoints[left_idx * 2] * scale_x, keypoints[left_idx * 2 + 1] * scale_y);
                points2.emplace_back(keypoints[MODEL_IMAGE_SIZE * 2 + right_idx * 2] * scale_x,
                                   keypoints[MODEL_IMAGE_SIZE * 2 + right_idx * 2 + 1] * scale_y);
            }
        }
    }
    std::cout << "Matches: " << points1.size() << "\n";

    cv::Mat K = load_calibration(dir_path + "calib.txt");
    cv::Mat R_est, T_est;
    estimate_pose(points1, points2, K, R_est, T_est);

    // Triangulate 3D points from image0 and image1
    cv::Mat R_rel = R_est.t();
    cv::Mat T_rel = -R_rel * T_est;
    cv::Mat P0 = K * cv::Mat::eye(3, 4, CV_64F);
    cv::Mat P1(3, 4, CV_64F);
    R_rel.copyTo(P1(cv::Rect(0,0,3,3)));
    T_rel.copyTo(P1(cv::Rect(3,0,1,3)));
    P1 = K * P1;
    cv::Mat points1_mat(2, points1.size(), CV_64F), points2_mat(2, points2.size(), CV_64F);
    for (size_t i = 0; i < points1.size(); ++i) {
        points1_mat.at<double>(0,i) = points1[i].x; points1_mat.at<double>(1,i) = points1[i].y;
        points2_mat.at<double>(0,i) = points2[i].x; points2_mat.at<double>(1,i) = points2[i].y;
    }
    cv::Mat points3D;
    cv::triangulatePoints(P0, P1, points1_mat, points2_mat, points3D);
    std::vector<cv::Point3d> points3D_vec;
    for (int i = 0; i < points3D.cols; ++i) {
        double w = points3D.at<double>(3,i);
        if (w != 0) points3D_vec.emplace_back(points3D.at<double>(0,i)/w, points3D.at<double>(1,i)/w, points3D.at<double>(2,i)/w);
    }

    // Inference on image1 and image2
    std::vector<int64_t> keypoints12, matches12;
    std::vector<float> scores12;
    infer.runInference(dir_path + "image_0/000001.png", dir_path + "image_0/000002.png", keypoints12, matches12, scores12);
    std::vector<cv::Point2f> points1_second, points2_second;
    for (size_t m = 0; m < scores12.size() && m * 3 + 2 < matches12.size(); ++m) {
        if (scores12[m] >= SCORE_THRESHOLD && matches12[m * 3] == 0) {
            int left_idx = matches12[m * 3 + 1];
            int right_idx = matches12[m * 3 + 2];
            if (left_idx >= 0 && left_idx < MODEL_IMAGE_SIZE && right_idx >= 0 && right_idx < MODEL_IMAGE_SIZE) {
                points1_second.emplace_back(keypoints12[left_idx * 2] * scale_x, keypoints12[left_idx * 2 + 1] * scale_y);
                points2_second.emplace_back(keypoints12[MODEL_IMAGE_SIZE * 2 + right_idx * 2] * scale_x,
                                         keypoints12[MODEL_IMAGE_SIZE * 2 + right_idx * 2 + 1] * scale_y);
            }
        }
    }

    // Associate 3D points with matches in image2
    std::vector<cv::Point3d> points3D_for_pnp;
    std::vector<cv::Point2f> points2_for_pnp;
    for (size_t m = 0; m < points1_second.size(); ++m) {
        double min_dist = std::numeric_limits<double>::max();
        int best_idx = -1;
        for (size_t i = 0; i < points2.size(); ++i) {
            double dist = cv::norm(points2[i] - points1_second[m]);
            if (dist < min_dist) { min_dist = dist; best_idx = i; }
        }
        if (min_dist < 1.0 && best_idx >= 0) {
            points3D_for_pnp.push_back(points3D_vec[best_idx]);
            points2_for_pnp.push_back(points2_second[m]);
        }
    }

    // PnP to estimate pose of image2
    cv::Mat rvec, tvec;
    cv::solvePnP(points3D_for_pnp, points2_for_pnp, K, cv::Mat(), rvec, tvec, false, cv::SOLVEPNP_ITERATIVE);
    cv::Mat R_pnp; cv::Rodrigues(rvec, R_pnp);
    cv::Mat R_pnp_inv = R_pnp.t();
    cv::Mat T_pnp_inv = -R_pnp_inv * tvec;

    // Load poses for 3 images
    auto poses = load_poses(dir_path + "00.txt", 3);
    cv::Mat R_gt = poses[1](cv::Rect(0, 0, 3, 3));
    cv::Mat T_gt = poses[1](cv::Rect(3, 0, 1, 3));
    cv::Mat R_gt_pnp = poses[2](cv::Rect(0, 0, 3, 3));
    cv::Mat T_gt_pnp = poses[2](cv::Rect(3, 0, 1, 3));

    std::cout << "Estimated T: " << T_est.t() << "\n";
    std::cout << "Ground Truth T: " << T_gt.t() << "\n";
    std::cout << "Rotation Error: " << compute_rotation_error(R_est, R_gt) << " deg\n";
    std::cout << "Translation Error: " << compute_translation_error(T_est, T_gt) << " deg\n";
    std::cout << "PnP Estimated T: " << T_pnp_inv.t() << "\n";
    std::cout << "PnP Ground Truth T: " << T_gt_pnp.t() << "\n";
    std::cout << "PnP Rotation Error: " << compute_rotation_error(R_pnp_inv, R_gt_pnp) << " deg\n";
    std::cout << "PnP Translation Error: " << compute_translation_error(T_pnp_inv, T_gt_pnp) << " deg\n";

    visualize_poses(R_est, T_est, R_gt, T_gt);
    return 0;
}